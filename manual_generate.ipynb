{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified: 'code/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Workspace\\Tutorial\\BGCA\\manual_generate.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/manual_generate.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/manual_generate.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/manual_generate.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m os\u001b[39m.\u001b[39;49mchdir(\u001b[39m'\u001b[39;49m\u001b[39mcode/\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/manual_generate.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/manual_generate.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'code/'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import re\n",
    "import os\n",
    "os.chdir('code/')\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import (AdamW, AutoModelForSeq2SeqLM,\n",
    "                          AutoTokenizer, get_linear_schedule_with_warmup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *\n",
    "from data_utils import (ABSADataset, filter_none, filter_invalid,\n",
    "                        get_dataset, get_inputs, normalize_augment)\n",
    "from model_utils import (prepare_constrained_tokens, prepare_tag_tokens)\n",
    "from main import *\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Workspace\\\\Tutorial\\\\BGCA\\\\code'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(adam_epsilon=1e-08, beam=1, clear_model=True, commit=None, data_dir='../outputs/aste/cross_domain/run_aste/seed-42/laptop14-rest14/data', data_gene=True, data_gene_aug_num=None, data_gene_aug_ratio=None, data_gene_decode=None, data_gene_epochs=25, data_gene_extract=True, data_gene_extract_epochs=25, data_gene_extract_none_remove_ratio=0, data_gene_min_length=0, data_gene_none_remove_ratio=0, data_gene_none_word_num=1, data_gene_num_beam=1, data_gene_same_model=False, data_gene_top_p=0.9, data_gene_wt_constrained=True, dataset='cross_domain', device='cuda', do_eval=True, do_train=True, eval_batch_size=16, extract_model=None, gene_model=None, gradient_accumulation_steps=2, inference_dir='../outputs/aste/cross_domain/run_aste/seed-42/laptop14-rest14/inference', init_tag='english', learning_rate=0.0003, max_seq_length=128, model_filter=True, model_filter_skip_none=False, model_name_or_path='../outputs/aste/cross_domain/run_aste/seed-42/laptop14-rest14/checkpoint-e24', n_gpu='0', n_runs=1, name='run_aste', no_greedy=False, nrows=None, num_train_epochs=25, ori_tok_len=32100, output_dir='../outputs/aste/cross_domain/run_aste', paradigm='extraction-universal', pseudo=False, pseudo_skip_none=False, runned_folder=None, save_best=True, save_last_k=3, score_dir='../outputs/aste/cross_domain/run_aste/seed-42/laptop14-rest14/score', seed=42, seed_dir='../outputs/aste/cross_domain/run_aste/seed-42/laptop14-rest14', source_domain='laptop14', target_domain=['rest14'], task='aste', train_batch_size=2, train_by_pair=True, use_same_model=True, warmup_steps=0.0, weight_decay=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Load arguments from the JSON file\n",
    "\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "arg_path = '../outputs/aste/cross_domain/run_aste/seed-42/laptop14-rest14/args.json'\n",
    "with open(arg_path, 'r') as file:\n",
    "    args_dict = json.load(file)\n",
    "\n",
    "# Create a namespace from the dictionary\n",
    "args = argparse.Namespace(**args_dict)\n",
    "\n",
    "args.model_name_or_path = '../outputs/aste/cross_domain/run_aste/seed-42/laptop14-rest14/checkpoint-e24'\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name_or_path).to(args.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'bread', 'is', 'top', 'notch', 'as', 'well', '.'] [([1], [3, 4], 'POS')]\n"
     ]
    }
   ],
   "source": [
    "test_sents, _ = read_line_examples_from_file(f\"{args.data_dir}/test.txt\")\n",
    "\n",
    "for x, y in zip(test_sents, _):\n",
    "    print(x, y)\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Workspace\\\\Tutorial\\\\BGCA\\\\code'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.data_dir\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_inputs_and_targets() --> The bread is top notch as well . ==> <pos> bread <opinion> top notch\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_dataset = get_dataset(args, task=args.task, data_type=\"test\", tokenizer=tokenizer)\n",
    "\n",
    "inputs, targets = get_inputs_and_targets(args, task=args.task, data_type=\"test\")\n",
    "# inputs, targets = get_inputs_and_targets(args, task=args.task, data_type='thai_data/laptop14/th/googletrans/test_aste_processed.txt')\n",
    "\n",
    "print(f'get_inputs_and_targets() --> {inputs[0]} ==> {targets[0]}')\n",
    "\n",
    "# test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_input = ['เวลาบูตเร็วสุด ๆ จาก 35 วินาทีถึง 1 นาที']\n",
    "th_target = ['<pos> เวลาบูต <opinion> เร็ว']\n",
    "th_input = ['เวลา บูต เร็ว สุด ๆ จาก 35 วินาที ถึง 1 นาที', 'เป็นเรื่องดีมากที่ไม่ต้องกังวลเกี่ยวกับเรื่องนี้และค่าใช้จ่ายเพิ่มเติมที่มาพร้อมกับการป้องกันไวรัสที่จำเป็นบนพีซี']\n",
    "th_target = ['','']\n",
    "en_input = ['The bread is top notch as well .']\n",
    "en_target = ['<pos> bread <opinion> top notch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_ids': tensor([   259, 158088, 150961,   1549,  83119, 169222,  34518, 117301,   2091,\n",
       "         119228,  58618,   1549, 220596, 237063, 182034,   1549, 130025,  17383,\n",
       "          57716,  25851,      1,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0]),\n",
       " 'source_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'target_ids': tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'target_mask': tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# temp_dataset = ABSADataset(args, tokenizer, inputs=inputs, targets=targets)\n",
    "temp_dataset = ABSADataset(args, tokenizer, inputs=th_input, targets=th_target)\n",
    "\n",
    "# temp_dataset = ABSADataset(args, tokenizer, inputs=en_input, targets=en_target)\n",
    "\n",
    "temp_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_new(args, dataset, model, tokenizer, name, is_constrained=False, constrained_vocab=None, keep_mask=False, **decode_dict):\n",
    "    dataloader = DataLoader(dataset, batch_size=args.eval_batch_size, num_workers=4)\n",
    "\n",
    "    if keep_mask:\n",
    "        # can't skip special directly, will lose extra_id\n",
    "        unwanted_tokens = [tokenizer.eos_token, tokenizer.unk_token, tokenizer.pad_token]\n",
    "        unwanted_ids = tokenizer.convert_tokens_to_ids(unwanted_tokens)\n",
    "        def filter_decode(ids):\n",
    "            ids = [i for i in ids if i not in unwanted_ids]\n",
    "            tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "            sentence = tokenizer.convert_tokens_to_string(tokens)\n",
    "            return sentence\n",
    "\n",
    "    # inference\n",
    "    inputs, outputs, targets = [], [], []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            if is_constrained:\n",
    "                prefix_fn_obj = Prefix_fn_cls(tokenizer, constrained_vocab, batch['source_ids'].to(args.device))  # need fix\n",
    "                prefix_fn = lambda batch_id, sent: prefix_fn_obj.get(batch_id, sent)\n",
    "            else:\n",
    "                prefix_fn = None\n",
    "\n",
    "            outs_dict = model.generate(input_ids=batch['source_ids'].to(args.device),\n",
    "                                        attention_mask=batch['source_mask'].to(args.device),\n",
    "                                        max_length=128,\n",
    "                                        prefix_allowed_tokens_fn=prefix_fn,\n",
    "                                        output_scores=True,\n",
    "                                        return_dict_in_generate=True,\n",
    "                                        **decode_dict,\n",
    "                                        )\n",
    "            outs = outs_dict[\"sequences\"]\n",
    "\n",
    "            if keep_mask:\n",
    "                input_ = [filter_decode(ids) for ids in batch[\"source_ids\"]]\n",
    "                dec = [filter_decode(ids) for ids in outs]\n",
    "                target = [filter_decode(ids) for ids in batch[\"target_ids\"]]\n",
    "            else:\n",
    "                input_ = [tokenizer.decode(ids, skip_special_tokens=True) for ids in batch[\"source_ids\"]]\n",
    "                dec = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
    "                target = [tokenizer.decode(ids, skip_special_tokens=True) for ids in batch[\"target_ids\"]]\n",
    "\n",
    "            inputs.extend(input_)\n",
    "            outputs.extend(dec)\n",
    "            targets.extend(target)\n",
    "\n",
    "    # decode_txt = \"constrained\" if is_constrained else \"greedy\"\n",
    "    # with open(os.path.join(args.inference_dir, f\"{name}_{decode_txt}_output.txt\"), \"w\") as f:\n",
    "    #     for i, o in enumerate(outputs):\n",
    "    #         f.write(f\"{inputs[i]} ===> {o}\\n\")\n",
    "\n",
    "    # return inputs, outputs, targets\n",
    "    return inputs, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text:เวลา บูต เร็ว สุด ๆ จาก 35 วินาที ถึง 1 นาที\n",
      "predict(gene) text:<neu> บูต <opinion> เร็ว\n",
      "input text:เป็นเรื่องดีมากที่ไม่ต้องกังวลเกี่ยวกับเรื่องนี้และค่าใช้จ่ายเพิ่มเติมที่มาพร้อมกับการป้องกันไวรัสที่จําเป็นบนพีซี\n",
      "predict(gene) text:<pos> การป้องกัน virus <opinion> ดีมาก\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_infer, output_infer = infer_new(\n",
    "        args, temp_dataset, model, tokenizer, name=f\"wtf\",\n",
    "        # is_constrained=True, \n",
    "        is_constrained=False, \n",
    "        constrained_vocab=prepare_constrained_tokens(tokenizer, args.task, args.paradigm),\n",
    "    )\n",
    "\n",
    "\n",
    "for x,y in zip(input_infer, output_infer):\n",
    "    print(f'input text:{x}')\n",
    "    print(f'predict(gene) text:{y}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
