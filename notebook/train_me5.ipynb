{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.18\n",
      "Name: transformers\n",
      "Version: 4.18.0\n",
      "Name: sentencepiece\n",
      "Version: 0.1.96\n",
      "Name: pytorch-lightning\n",
      "Version: 0.8.1\n",
      "Name: editdistance\n",
      "Version: 0.6.0\n",
      "Name: scikit-learn\n",
      "Version: 0.24.2\n",
      "Name: numpy\n",
      "Version: 1.22.3\n",
      "Name: tqdm\n",
      "Version: 4.64.0\n",
      "\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\workspace\\tutorial\\bgca\\.conda\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# check python version & nessessary package\n",
    "!python --version\n",
    "\n",
    "!pip show transformers sentencepiece pytorch_lightning editdistance scikit-learn numpy tqdm | python -c \"import sys, re; print(''.join(line for line in sys.stdin if re.search(r'Name|Version', line)), end='\\t')\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# path to folder 'code' in BGCA repo\n",
    "os.chdir('../code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Workspace\\\\Tutorial\\\\BGCA\\\\code'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: main.py [-h] --task TASK [--device DEVICE] --dataset DATASET\n",
      "               [--data_dir DATA_DIR] [--model_name_or_path MODEL_NAME_OR_PATH]\n",
      "               --paradigm PARADIGM [--do_train] [--do_eval] [--name NAME]\n",
      "               [--commit COMMIT] [--save_last_k SAVE_LAST_K] [--n_runs N_RUNS]\n",
      "               [--clear_model] [--save_best] [--nrows NROWS] [--train_by_pair]\n",
      "               [--target_domain TARGET_DOMAIN] [--no_greedy] [--beam BEAM]\n",
      "               [--max_seq_length MAX_SEQ_LENGTH] [--n_gpu N_GPU]\n",
      "               [--train_batch_size TRAIN_BATCH_SIZE]\n",
      "               [--eval_batch_size EVAL_BATCH_SIZE]\n",
      "               [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "               [--learning_rate LEARNING_RATE]\n",
      "               [--num_train_epochs NUM_TRAIN_EPOCHS] [--seed SEED]\n",
      "               [--weight_decay WEIGHT_DECAY] [--adam_epsilon ADAM_EPSILON]\n",
      "               [--warmup_steps WARMUP_STEPS] [--init_tag INIT_TAG]\n",
      "               [--data_gene] [--data_gene_epochs DATA_GENE_EPOCHS]\n",
      "               [--data_gene_extract]\n",
      "               [--data_gene_extract_epochs DATA_GENE_EXTRACT_EPOCHS]\n",
      "               [--data_gene_none_remove_ratio DATA_GENE_NONE_REMOVE_RATIO]\n",
      "               [--data_gene_none_word_num DATA_GENE_NONE_WORD_NUM]\n",
      "               [--data_gene_extract_none_remove_ratio DATA_GENE_EXTRACT_NONE_REMOVE_RATIO]\n",
      "               [--data_gene_same_model] [--data_gene_wt_constrained]\n",
      "               [--use_same_model] [--model_filter] [--model_filter_skip_none]\n",
      "               [--data_gene_decode DATA_GENE_DECODE]\n",
      "               [--data_gene_top_p DATA_GENE_TOP_P]\n",
      "               [--data_gene_num_beam DATA_GENE_NUM_BEAM]\n",
      "               [--data_gene_min_length DATA_GENE_MIN_LENGTH]\n",
      "               [--extract_model EXTRACT_MODEL] [--gene_model GENE_MODEL]\n",
      "               [--runned_folder RUNNED_FOLDER]\n",
      "               [--data_gene_aug_num DATA_GENE_AUG_NUM]\n",
      "               [--data_gene_aug_ratio DATA_GENE_AUG_RATIO] [--pseudo]\n",
      "               [--pseudo_skip_none]\n",
      "main.py: error: unrecognized arguments: \\\n"
     ]
    }
   ],
   "source": [
    "python main.py --task aste --name me5-base  --seed 42  --dataset cross_domain  --model_name_or_path intfloat/multilingual-e5-base  --paradigm extraction-universal  --n_gpu 0  --train_batch_size 16  --gradient_accumulation_steps 2  --eval_batch_size 16  --learning_rate 3e-4  --num_train_epochs 25  --save_last_k 3  --n_runs 1  --clear_model  --save_best  --data_gene  --data_gene_extract  --data_gene_extract_epochs 25  --data_gene_epochs 25  --init_tag english  --do_train  --do_eval  --use_same_model  --data_gene_wt_constrained  --model_filter  --train_by_pair \n",
    "\n",
    "python main.py --task aste --name me5-base  --seed 42  --dataset cross_domain  --model_name_or_path intfloat/multilingual-e5-base  --paradigm extraction-universal  --n_gpu 1  --train_batch_size 16  --gradient_accumulation_steps 2  --eval_batch_size 16  --learning_rate 3e-4  --num_train_epochs 5  --save_last_k 3  --n_runs 1  --clear_model  --save_best  --data_gene  --data_gene_extract  --data_gene_extract_epochs 5  --data_gene_epochs 5  --init_tag english  --do_train  --do_eval  --use_same_model  --data_gene_wt_constrained  --model_filter  --train_by_pair "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "import os\n",
    "os.chdir('../code/')\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# from tqdm import tqdm, trange\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from transformers import (AdamW, AutoModelForSeq2SeqLM, AutoModel,\n",
    "                          AutoTokenizer, get_linear_schedule_with_warmup)\n",
    "from constants import *\n",
    "from data_utils import (ABSADataset, filter_none, filter_invalid,\n",
    "                        get_dataset, get_inputs, normalize_augment)\n",
    "from model_utils import (prepare_constrained_tokens, prepare_tag_tokens)\n",
    "from main import *\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(adam_epsilon=1e-08, beam=1, clear_model=True, commit=None, data_dir='../outputs/aste/cross_domain\\\\me5-base/seed-42/rest14-laptop14\\\\data', data_gene=True, data_gene_aug_num=None, data_gene_aug_ratio=None, data_gene_decode=None, data_gene_epochs=25, data_gene_extract=True, data_gene_extract_epochs=25, data_gene_extract_none_remove_ratio=0, data_gene_min_length=0, data_gene_none_remove_ratio=0, data_gene_none_word_num=1, data_gene_num_beam=1, data_gene_same_model=False, data_gene_top_p=0.9, data_gene_wt_constrained=True, dataset='cross_domain', device='cuda', do_eval=True, do_train=True, eval_batch_size=16, extract_model=None, gene_model=None, gradient_accumulation_steps=2, inference_dir='../outputs/aste/cross_domain\\\\me5-base/seed-42/rest14-laptop14\\\\inference', init_tag='english', learning_rate=0.0003, max_seq_length=128, model_filter=True, model_filter_skip_none=False, model_name_or_path='../outputs/aste/cross_domain/me5-base/seed-42/rest14-laptop14/extract_aste-model', n_gpu='0', n_runs=1, name='me5-base', no_greedy=False, nrows=None, num_train_epochs=25, ori_tok_len=32100, output_dir='../outputs/aste/cross_domain\\\\me5-base', paradigm='extraction-universal', pseudo=False, pseudo_skip_none=False, runned_folder=None, save_best=True, save_last_k=3, score_dir='../outputs/aste/cross_domain\\\\me5-base/seed-42/rest14-laptop14\\\\score', seed=42, seed_dir='../outputs/aste/cross_domain\\\\me5-base/seed-42/rest14-laptop14', source_domain='rest14', target_domain=['laptop14'], task='aste', train_batch_size=16, train_by_pair=True, use_same_model=True, warmup_steps=0.0, weight_decay=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Load arguments from the JSON file\n",
    "\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "arg_path = '../outputs/aste/cross_domain/me5-base/seed-42/rest14-laptop14/args.json'\n",
    "with open(arg_path, 'r') as file:\n",
    "    args_dict = json.load(file)\n",
    "\n",
    "# Create a namespace from the dictionary\n",
    "args = argparse.Namespace(**args_dict)\n",
    "\n",
    "args.model_name_or_path = '../outputs/aste/cross_domain/me5-base/seed-42/rest14-laptop14/extract_aste-model'\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(args.model_name_or_path).to(args.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = [\n",
    "    \"เป็นร้านที่ไปยากนิดนึง หากไม่คุ้นเคยเส้นทาง เห็นมีหลายคนหลงทาง เป็นร้านอาหารสำหรับนักท่องเที่ยว หรือเอาแขกต่างชาติมา พื้นที่ร้านมหึมาและดูอลังการ เน้นบรรยากาศและการตกแต่งเป็นส่วนใหญ่ ส่วนมากจะเป็นของที่สะสม ส่วนอาหารถือว่าพอได้ แต่ยังดูธรรมดาหากเปรียบเทียบกับร้านอาหารไทยแนวเดียวกัน รวมถึงราคาก็จะสูงหน่อย\"\n",
    "]\n",
    "\n",
    "target_list = [\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Workspace\\Tutorial\\BGCA\\.conda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = ABSADataset(args, tokenizer, inputs=input_list, targets=target_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_new(args, dataset, model, tokenizer, name=None, is_constrained=False, constrained_vocab=None, keep_mask=False, **decode_dict):\n",
    "    dataloader = DataLoader(dataset, batch_size=args.eval_batch_size, num_workers=4)\n",
    "\n",
    "    if keep_mask:\n",
    "        # can't skip special directly, will lose extra_id\n",
    "        unwanted_tokens = [tokenizer.eos_token, tokenizer.unk_token, tokenizer.pad_token]\n",
    "        unwanted_ids = tokenizer.convert_tokens_to_ids(unwanted_tokens)\n",
    "        def filter_decode(ids):\n",
    "            ids = [i for i in ids if i not in unwanted_ids]\n",
    "            tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "            sentence = tokenizer.convert_tokens_to_string(tokens)\n",
    "            return sentence\n",
    "\n",
    "    # inference\n",
    "    inputs, outputs, targets = [], [], []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Generating'):\n",
    "            if is_constrained:\n",
    "                prefix_fn_obj = Prefix_fn_cls(tokenizer, constrained_vocab, batch['source_ids'].to(args.device))  # need fix\n",
    "                prefix_fn = lambda batch_id, sent: prefix_fn_obj.get(batch_id, sent)\n",
    "            else:\n",
    "                prefix_fn = None\n",
    "\n",
    "            outs_dict = model.generate(input_ids=batch['source_ids'].to(args.device),\n",
    "                                        attention_mask=batch['source_mask'].to(args.device),\n",
    "                                        # max_length=128,\n",
    "                                        # prefix_allowed_tokens_fn=prefix_fn,\n",
    "                                        # output_scores=True,\n",
    "                                        # return_dict_in_generate=True,\n",
    "                                        # **decode_dict,\n",
    "                                        )\n",
    "            outs = outs_dict[\"sequences\"]\n",
    "\n",
    "            if keep_mask:\n",
    "                input_ = [filter_decode(ids) for ids in batch[\"source_ids\"]]\n",
    "                dec = [filter_decode(ids) for ids in outs]\n",
    "                target = [filter_decode(ids) for ids in batch[\"target_ids\"]]\n",
    "            else:\n",
    "                input_ = [tokenizer.decode(ids, skip_special_tokens=True) for ids in batch[\"source_ids\"]]\n",
    "                dec = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
    "                target = [tokenizer.decode(ids, skip_special_tokens=True) for ids in batch[\"target_ids\"]]\n",
    "\n",
    "            inputs.extend(input_)\n",
    "            outputs.extend(dec)\n",
    "            targets.extend(target)\n",
    "\n",
    "    # decode_txt = \"constrained\" if is_constrained else \"greedy\"\n",
    "    # with open(os.path.join(args.inference_dir, f\"{name}_{decode_txt}_output.txt\"), \"w\") as f:\n",
    "    #     for i, o in enumerate(outputs):\n",
    "    #         f.write(f\"{inputs[i]} ===> {o}\\n\")\n",
    "\n",
    "    # return inputs, outputs, targets\n",
    "    return inputs, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35de68aeadf4946b4a98c03329649d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 128, but ``max_length`` is set to 128. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Workspace\\Tutorial\\BGCA\\notebook\\train_me5.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/notebook/train_me5.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m input_infer, output_infer \u001b[39m=\u001b[39m infer_new(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/notebook/train_me5.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m         args, dataset, model, tokenizer, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/notebook/train_me5.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39m# name=f\"thai-pred\",\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/notebook/train_me5.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39m# is_constrained=True, \u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/notebook/train_me5.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         is_constrained\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/notebook/train_me5.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         constrained_vocab\u001b[39m=\u001b[39;49mprepare_constrained_tokens(tokenizer, args\u001b[39m.\u001b[39;49mtask, args\u001b[39m.\u001b[39;49mparadigm),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/notebook/train_me5.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/notebook/train_me5.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m x,y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(input_infer, output_infer):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/notebook/train_me5.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39minput text:\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Workspace\\Tutorial\\BGCA\\notebook\\train_me5.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/notebook/train_me5.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/notebook/train_me5.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     prefix_fn \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/notebook/train_me5.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m outs_dict \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49mbatch[\u001b[39m'\u001b[39;49m\u001b[39msource_ids\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mto(args\u001b[39m.\u001b[39;49mdevice),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/notebook/train_me5.ipynb#X13sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m                             attention_mask\u001b[39m=\u001b[39;49mbatch[\u001b[39m'\u001b[39;49m\u001b[39msource_mask\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mto(args\u001b[39m.\u001b[39;49mdevice),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/notebook/train_me5.ipynb#X13sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m                             max_length\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/notebook/train_me5.ipynb#X13sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m                             prefix_allowed_tokens_fn\u001b[39m=\u001b[39;49mprefix_fn,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/notebook/train_me5.ipynb#X13sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m                             output_scores\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/notebook/train_me5.ipynb#X13sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m                             return_dict_in_generate\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/notebook/train_me5.ipynb#X13sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m                             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdecode_dict,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/notebook/train_me5.ipynb#X13sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m                             )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/notebook/train_me5.ipynb#X13sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m outs \u001b[39m=\u001b[39m outs_dict[\u001b[39m\"\u001b[39m\u001b[39msequences\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/Tutorial/BGCA/notebook/train_me5.ipynb#X13sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mif\u001b[39;00m keep_mask:\n",
      "File \u001b[1;32mc:\\Workspace\\Tutorial\\BGCA\\.conda\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Workspace\\Tutorial\\BGCA\\.conda\\lib\\site-packages\\transformers\\generation_utils.py:1254\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[0;32m   1249\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnum_return_sequences has to be 1, but is \u001b[39m\u001b[39m{\u001b[39;00mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m when doing greedy search.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1251\u001b[0m         )\n\u001b[0;32m   1253\u001b[0m     \u001b[39m# 10. run greedy search\u001b[39;00m\n\u001b[1;32m-> 1254\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[0;32m   1255\u001b[0m         input_ids,\n\u001b[0;32m   1256\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[0;32m   1257\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[0;32m   1258\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[0;32m   1259\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[0;32m   1260\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[0;32m   1261\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[0;32m   1262\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[0;32m   1263\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[0;32m   1264\u001b[0m     )\n\u001b[0;32m   1266\u001b[0m \u001b[39melif\u001b[39;00m is_sample_gen_mode:\n\u001b[0;32m   1267\u001b[0m     \u001b[39m# 10. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(\n\u001b[0;32m   1269\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k, top_p\u001b[39m=\u001b[39mtop_p, typical_p\u001b[39m=\u001b[39mtypical_p, temperature\u001b[39m=\u001b[39mtemperature, num_beams\u001b[39m=\u001b[39mnum_beams\n\u001b[0;32m   1270\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Workspace\\Tutorial\\BGCA\\.conda\\lib\\site-packages\\transformers\\generation_utils.py:1649\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   1646\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1647\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n\u001b[1;32m-> 1649\u001b[0m next_token_logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39;49mlogits[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[0;32m   1651\u001b[0m \u001b[39m# Store scores, attentions and hidden_states when required\u001b[39;00m\n\u001b[0;32m   1652\u001b[0m \u001b[39mif\u001b[39;00m return_dict_in_generate:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "input_infer, output_infer = infer_new(\n",
    "        args, dataset, model, tokenizer, \n",
    "        # name=f\"thai-pred\",\n",
    "        # is_constrained=True, \n",
    "        is_constrained=False, \n",
    "        constrained_vocab=prepare_constrained_tokens(tokenizer, args.task, args.paradigm),\n",
    "    )\n",
    "\n",
    "\n",
    "for x,y in zip(input_infer, output_infer):\n",
    "    print(f'input text:{x}')\n",
    "    print(f'predict(gene) text:{y}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
